{"cells":[{"cell_type":"markdown","metadata":{"id":"GItxU96mTk16"},"source":["# Matemática para Ciencia de los Datos\n","# Trabajo Práctico 6\n","\n","Profesor: Juan Luis Crespo Mariño (basado en trabajo previo de Luis Alexánder Calvo Valverde)\n","\n","Instituto Tecnológico de Costa Rica,\n","\n","Programa Ciencia de Datos\n","\n","---\n","\n","Fecha de entrega: 2 de septiembre, hora límite las 6:00 pm.\n","\n","Medio de entrega: Por medio del TEC-Digital.\n","\n","Entregables: Un archivo jupyter ( .IPYNB ).\n","\n","Estudiante:\n","1. **Nombre_Estudiante**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ce5WVgoNSSAf"},"source":["## Ejercicio 1 (50 puntos)\n","\n","\n","\n","El algoritmo del descenso de gradiente sigue la idea de modificar el punto óptimo estimado de forma iterativa. Para una función en una\n","variable $f\\left(x\\right)$, la estimación del punto óptimo en una iteración $i+1$ está dada por:\n","\n","\\begin{equation}\n","x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n","\\end{equation}\n","\n","donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimización iterativa sigue\n","la dirección de la derivada. Para la optimización de una función multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posición óptima se estima usando el vector gradiente:\n","\n","\\begin{equation}\n","\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n","\\end{equation}\n","\n","Para la función:\n","\n","\\begin{equation}\n","f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n","\\end{equation}\n","\n","Implemente la función en python denominada:\n","\n","$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n","\n","donde los parámetros corresponden a:\n","\n","* tasa_aprendizaje: es el $\\alpha$\n","* iteraciones: es el máximo número de iteraciones a ejecutar\n","* xy: es el vector con los dos valores iniciales [x,y]\n","* tolerancia: es el valor mínimo para un cambio entre iteración. Si la función de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteración.\n","\n","**Nota:**\n","1. Para iniciar la implementación puede utilizar el código en el cuaderno \"070_1_LACV_Optimizacion\".\n","1. Cada iteración le generará un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la función de pérdida en ese punto, evalúelo en la función inicial ($x^{2}-y^{2}$) para saber si aumentó o disminuyó.\n","\n","\n","\n","---\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvpiG31kSSAg"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"XRBmuhcjSSAh"},"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"7GoSa-9MSSAh"},"source":["## Ejercicio 2\n","\n","Para la función  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^3 + x_{2}^3 - 3x_{1}^2 - 3x_{2}^2 - 3x_{1}x_{2} +2 $\n","\n","Realice lo siguiente:\n","\n","1. En una celda de texto:\n","\n"," - Calcule el vector gradiente. **(15 puntos)**\n","\n"," - Calcule la matriz Hessiana. **(15 puntos)**\n","\n","2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n","  - Evalúela en los puntos que tentativamente anulen el gradiente de la misma (pueden usar python o métodos similares para resolver sistemas de ecuaciones, si lo necesitan).\n","  - Luego aplique el criterio de la segunda derivada parcial ¿qué conclusiones saca para esos puntos?\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNCDadfESSAh"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}